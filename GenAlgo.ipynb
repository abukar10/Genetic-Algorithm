{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2a68c8-ac2f-48db-a94f-b00922bad8d8",
   "metadata": {},
   "source": [
    "## GENETIC ALGORITHM\n",
    "### Classification of Farsi Vowel Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86cda13e-cb45-41da-986b-ce35178ff685",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"NN_pop2.jpg\" style=\"width:900px; height:400px;\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "display(HTML('<img src=\"NN_pop2.jpg\" style=\"width:900px; height:400px;\">'))  # Set your desired dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ece08-b43b-404d-bbf9-3f85cec7a023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48270926-485e-440e-ab05-e3d69052d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import librosa\n",
    "import scipy.io\n",
    "from scipy.signal.windows import hann\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# We suggest your GA uses at least these hyper-parameters\n",
    "# The values stated here are suboptimal and you should tune them\n",
    "# You should also consider adding more hyper-parameters\n",
    "population_size = 5\n",
    "num_generations = 2\n",
    "num_parents = 2\n",
    "\n",
    "\n",
    "\n",
    "# these are other global hyper-parameters that apply across all neural networks.\n",
  
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2d15e2e-9162-494f-b236-4dfb85d24e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### Data Preperation ##########################################################\n",
    "\n",
  
    "def load_and_process_pcvc_data(directory='.', train_size=0.8, random_seed=42):\n",
    "    \"\"\"\n",
    "    Load, process, and split the PCVC dataset from .mat files into training and validation sets,\n",
    "    with randomized window slicing for the training data to expand the dataset size.\n",
    "\n",
    "    Instructions for data preparation:\n",
    "    1. Visit the Kaggle dataset page at https://www.kaggle.com/sabermalek/pcvcspeech\n",
    "    2. Download the dataset by clicking on the 'Download' button.\n",
    "    3. Extract the downloaded zip file.\n",
    "    4. Ensure all .mat files from the dataset are in the same directory as this script or specify the directory.\n",
    "\n",
    "    Parameters:\n",
    "    - directory: str, the directory where .mat files are located (default is the current directory).\n",
    "    - train_size: float, the proportion of the dataset to include in the train split.\n",
    "    - random_seed: int, the seed used for random operations to ensure reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - tr_data: np.array, training dataset.\n",
    "    - tr_labels: np.array, training labels.\n",
    "    - vl_data: np.array, validation dataset.\n",
    "    - vl_labels: np.array, validation labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # List all .mat files in the specified directory\n",
    "    all_mats = [file for file in os.listdir(directory) if file.endswith('.mat')]\n",
    "    raw_data = []\n",
    "    num_vowels = 6\n",
    "    ndatapoints_per_vowel = 299\n",
    "    labels = []\n",
    "\n",
    "    for idx, mat_file in enumerate(all_mats):\n",
    "        mat_path = os.path.join(directory, mat_file)\n",
    "        mat_data = np.squeeze(scipy.io.loadmat(mat_path)['x'])\n",
    "        raw_data.append(mat_data)\n",
    "        labels.append(np.repeat(np.arange(num_vowels)[np.newaxis], mat_data.shape[0], axis=0))\n",
    "\n",
    "    # Concatenate and reshape all data\n",
    "    raw_data, labels = np.concatenate(raw_data, axis=1), np.concatenate(labels, axis=1)\n",
    "    nreps, nvow, nsamps = raw_data.shape\n",
    "    raw_data = np.reshape(raw_data, (nreps * nvow, nsamps), order='F')\n",
    "    labels = np.reshape(labels, (nreps * nvow), order = 'F')\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    tr_data, vl_data, tr_labels, vl_labels = train_test_split(\n",
    "        raw_data, labels, train_size=train_size, random_state=random_seed, stratify=labels)\n",
    "    \n",
    "    # Define window size and function\n",
    "    window_size = 10000\n",
    "    window = hann(window_size)\n",
    "\n",
    "    # Process Training Data with random slicing\n",
    "    tr_data_processed = []\n",
    "    tr_labels_processed = []\n",
    "    for j in range(10): # repeat the tr data 10 times\n",
    "      for i, d in enumerate(tr_data):\n",
    "          start = np.random.randint(0, nsamps - window_size)\n",
    "          end = start + window_size\n",
    "          sliced = d[start:end] * window\n",
    "          resampled = librosa.resample(sliced, orig_sr=48000, target_sr=16000)\n",
    "          tr_data_processed.append(resampled)\n",
    "          tr_labels_processed.append(tr_labels[i])\n",
    "    tr_data = np.array(tr_data_processed)\n",
    "    tr_labels= np.array(tr_labels_processed)\n",
    "\n",
    "    # Process Validation Data with fixed slicing\n",
    "    vl_data = vl_data[:, 5000:15000] * window\n",
    "    vl_data = np.array([librosa.resample(d, orig_sr=48000, target_sr=16000) for d in vl_data])\n",
    "\n",
    "    # One-hot encode labels\n",
    "    tr_labels = np.eye(num_vowels)[tr_labels]\n",
    "    vl_labels = np.eye(num_vowels)[vl_labels]\n",
    "\n",
    "    return tr_data, tr_labels.astype('float'), vl_data, vl_labels.astype('float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c66a9f6-b39e-4a9c-a68d-8a808b84406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### Neural Network Architecture design ##########################################################\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines a neural network architecture dynamically based on a specified genome configuration.\n",
    "\n",
    "    The `Net` class constructs a neural network where each layer's configuration is dictated by the genome.\n",
    "    The network will always end with a linear layer with an output size of `K`, meant\n",
    "    to match the number of classes in the dataset.\n",
   
    "\n",
    "    Parameters:\n",
    "    - genome (list of dicts): Specifies the architecture of the neural network. Each dictionary in the list\n",
    "      represents a layer in the network and should include keys for 'num_neurons' (int), 'activation' (str),\n",
    "      and optionally 'dropout_rate' (float).\n",
    "    - D (int): The dimensionality of the input data. Defaults to 3.\n",
    "    - K (int): The number of output classes. Defaults to 4.\n",
    "\n",
    "    Attributes:\n",
    "    - network (nn.Sequential): The sequential container of network layers as specified by the genome.\n",
    "    \"\"\"\n",
    "    # we pass genome which is a dictionary of network configurations such as \n",
    "    # nr of neurons, activation function and dropout layer, K = 6 is the number of classes in \n",
    "    #..our Farsi Vowel classification\n",
    "    #reference:  https://learn.codesignal.com/preview/lessons/2237/building-networks-easily-with-sequential-models-in-pytorch\n",
    "    #reference:  https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html\n",
    "    #reference: https://discuss.pytorch.org/t/nn-sequential-multiple-arguments-in-dynamic-numbers-of-layers/96278\n",
    "    #reference: ChatGPT4o: correct errors related to randomizing the chromosom genenes. i.e if statement to dymamically pick\n",
    "    # the gene config, the neurons, dropout layer\n",
    "    \n",
    "    def __init__(self, genome, D=3334, K=6):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        input_features = D\n",
    "        for gene in genome:\n",
    "            \n",
    "            # add a fully connected layer\n",
    "            layers.append(nn.Linear(input_features, gene['num_neurons']))\n",
    "            # update input size for the next layer\n",
    "            input_features = gene['num_neurons']\n",
    "            \n",
    "            # Add activation function based on gene configuration\n",
    "            if gene['activation'] == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif gene['activation'] == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            elif gene['activation'] == 'sigmoid':\n",
    "                layers.append(nn.Sigmoid())\n",
    "            \n",
    "            # Add dropout layer if dropout rate is specified\n",
    "            if 'dropout_rate' in gene and gene['dropout_rate'] is not None:\n",
    "                layers.append(nn.Dropout(gene['dropout_rate']))\n",
    "        \n",
    "        # Add the final output layer with K units (one for each class)\n",
    "        layers.append(nn.Linear(input_features, K))\n",
    "             \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    # you shouldn't have to modify this method\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the neural network.\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): The input data tensor.\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: The output of the network after processing the input tensor through all the layers defined\n",
    "          in the `network` attribute.\n",
    "        \"\"\"\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8df4e6a-385d-49f8-b79a-2184b6775f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### Population ##########################################################\n",
    "\n",
    "def generate_initial_population(size, blueprint):\n",
    "    \"\"\"\n",
    "    Generates an initial population of neural network architectures based on a flexible blueprint.\n",
    "\n",
    "    Each individual in the population (or 'genome') consists of a randomly constructed neural network architecture.\n",
    "    The architecture is determined by randomly selecting from possible configurations specified in the blueprint.\n",
    "\n",
    "    Parameters:\n",
    "    - size (int): The number of neural network architectures to generate in the population.\n",
    "    - blueprint (dict): A dictionary specifying the possible configurations for neural network layers.\n",
    "      The blueprint can contain keys such as:\n",
    "      - 'n_layers' (int): The number of layers to include in each network architecture.\n",
    "      - 'neurons' (list): Possible numbers of neurons per layer.\n",
    "      - 'activations' (list): Possible activation functions.\n",
    "      - 'dropout' (list): Possible dropout rates, including None if dropout is not to be applied.\n",
    "\n",
    "    Returns:\n",
    "    - population (list of list of dicts): A list of neural network architectures, where each architecture\n",
    "      is represented as a list of dictionaries. Each dictionary defines the configuration of one layer.\n",
    "\n",
    "    Example:\n",
    "    >>> population = generate_initial_population(10, blueprint)\n",
    "    >>> len(population)\n",
    "    10\n",
    "    \"\"\"\n",
    "\n",
    "    population = []\n",
    "\n",
    "    for _ in range(size):\n",
    "        # Each network architecture is represented by a 'genome', which is a list of layers\n",
    "        genome = []\n",
    "        \n",
    "        # Determine the number of layers for this genome\n",
    "        n_layers = blueprint['n_layers']\n",
    "        \n",
    "        for _ in range(n_layers):\n",
    "            # Create a layer configuration by randomly selecting from the blueprint options\n",
    "            layer_config = {\n",
    "                'num_neurons': random.choice(blueprint['neurons']),\n",
    "                'activation': random.choice(blueprint['activations']),\n",
    "                'dropout_rate': random.choice(blueprint['dropout']) if 'dropout' in blueprint else None\n",
    "            }\n",
    "            genome.append(layer_config)\n",
    "\n",
    "        # Add this genome to the population\n",
    "        population.append(genome)\n",
    "    \n",
    "    return population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9523617-7adb-4a58-8827-ee2e865add0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### SELECTION Process ##########################################################\n",
    "\n",
    "def selection(population, fitnesses, num_parents):\n",
    "    \"\"\"\n",
    "    Selects the top-performing individuals from the population based on their fitness scores.\n",
    "\n",
    "    This function sorts the population by fitness in descending order and selects the top `num_parents`\n",
    "    individuals to form the next generation's parent group. This selection process ensures that individuals\n",
    "    with higher fitness have a higher probability of reproducing and passing on their genes.\n",
    "\n",
    "    Parameters:\n",
    "    - population (list of list of dicts): The population from which to select top individuals. Each individual\n",
    "      in the population is represented as a genome, which is a list of dictionaries where each dictionary\n",
    "      details the configuration of a neural network layer.\n",
    "    - fitnesses (list of floats): A list of fitness scores corresponding to each individual in the population.\n",
    "      Each fitness score should be a float indicating the performance of the associated individual.\n",
    "    - num_parents (int): The number of top-performing individuals to select for the next generation.\n",
    "\n",
    "    Returns:\n",
    "    - list of list of dicts: A list containing the genomes of the top-performing individuals selected from the\n",
    "      population.\n",
    "\n",
    "    Example:\n",
    "    >>> population = [[{'num_neurons': 32, 'activation': 'relu'}], [{'num_neurons': 16, 'activation': 'sigmoid'}]]\n",
    "    >>> fitnesses = [0.95, 0.88]\n",
    "    >>> selected = selection(population, fitnesses, 1)\n",
    "    >>> len(selected)\n",
    "    1\n",
    "    \"\"\"\n",
    "\n",
    "    # each individual in the population is paired with its fitness score\n",
    "    # and the result is a list of indivual_i and its fitness score, etc\n",
    "    pop_with_fitness = list(zip(population, fitnesses))\n",
    "    \n",
    "    # once we have the list of pairs, inviduals and their score we then ..\n",
    "    # ..sort these in descending ornder. \n",
    "    sorted_population = sorted(pop_with_fitness, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Select the top `num_parents` individuals based on their fitness\n",
    "    selected = [individual for individual, fitness in sorted_population[:num_parents]]\n",
    "    \n",
    "    return selected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1ad5eeb-b411-493b-9d11-6cb164dcdd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### Crossover Process ##########################################################\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    \"\"\"\n",
    "    Combines two parent genomes to create a new child genome through a crossover process of your choice.\n",
    "\n",
    "    Parameters:\n",
    "    - parent1 (list of dicts): The genome of the first parent.\n",
    "    - parent2 (list of dicts): The genome of the second parent.\n",
    "    - other keyword arguments as you may need\n",
    "\n",
    "    Returns:\n",
    "    - list of dicts: The genome of the child, formed by combining genes from the two parents.\n",
    "    \"\"\"\n",
    "    child = []\n",
    "    \n",
    "    # Single-point crossover\n",
    "    # Chose a point within the genome\n",
    "    # this method of cross over is very simple and takes only single feature from each parent\n",
    "    # this means that network characteris such as nr of neurons, or activation or dropout rate is picket\n",
    "    # from one parent and another from second parent to create a new offspring (New NN architecture) \n",
    "    # reference:  https://www.kdnuggets.com/understanding-and-implementing-genetic-algorithms-in-python\n",
    "    # refenence:  ChatGpt 4o: resolve errors related to function error\n",
    "    crossover_point = random.randint(1, len(parent1) - 1)  \n",
    "    # Combine genes from both parents\n",
    "    child = parent1[:crossover_point] + parent2[crossover_point:]   \n",
    "    \n",
    "    return child\n",
    "\n",
    "\n",
    "# reference: https://www.kdnuggets.com/understanding-and-implementing-genetic-algorithms-in-python\n",
    "def mutate(genome, mutation_rate=0.1, blueprint=None):\n",
    "    \"\"\"\n",
    "    Introduces random changes to a genome based on a specified mutation approach.\n",
    "\n",
    "    Parameters:\n",
    "    - genome (list of dicts): The genome to be mutated.\n",
    "    - mutation_rate (float): The probability of mutating each gene in the genome.\n",
    "    - blueprint (dict): Possible ranges for mutation (e.g., neuron counts, activations, dropout rates).\n",
    "\n",
    "    Returns:\n",
    "    - list of dicts: The mutated genome.\n",
    "    \"\"\"\n",
    "    for layer in genome:\n",
    "        if random.random() < mutation_rate:  # Only mutate with a certain probability\n",
    "            # Randomly mutate 'num_neurons'\n",
    "            if 'num_neurons' in layer:\n",
    "                layer['num_neurons'] = random.choice(blueprint['neurons'])\n",
    "\n",
    "            # Randomly mutate 'activation' function\n",
    "            if 'activation' in layer:\n",
    "                layer['activation'] = random.choice(blueprint['activations'])\n",
    "\n",
    "            # Randomly mutate 'dropout_rate'\n",
    "            if 'dropout_rate' in layer and blueprint['dropout']:\n",
    "                layer['dropout_rate'] = random.choice(blueprint['dropout'])\n",
    "                \n",
    "    return genome\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9e29946-be8e-4a36-8809-f6fde7fbf159",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### Fitness ###########################################################\n",
    "\n",
    "# You do not have to modify this function\n",
    "def compute_fitness(genome, train_loader, test_loader, criterion, lr=0.01, epochs=5, D=None, K=None):\n",
    "\n",
    "    # Create the model from the genome\n",
    "    model = Net(genome, D, K)\n",
    "\n",
    "    # suggested optimizer to train your models\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_batches = len(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        average_epoch_loss = epoch_loss / total_batches\n",
    "        print(f'Epoch {epoch + 1}/{epochs} complete. Average Training Loss: {average_epoch_loss:.4f}')\n",
    "        total_loss += epoch_loss\n",
    "\n",
    "    print(f'Training complete.')\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            target = target.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Evaluation complete. Accuracy: {accuracy:.4f} ({correct}/{total} correct)\\n')\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11887bd5-e0a8-46ba-8fd2-2254d279a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### Data: Training and validation Process ###########################################################\n",
    "\n",
    "# Load and process the PCVC dataset into training and validation sets.\n",
    "# This function will split the raw data, apply preprocessing like windowing and resampling,\n",
    "# and return processed training data (X, labels) and validation data (X_val, labels_val).\n",
    "X, labels, X_val, labels_val = load_and_process_pcvc_data()\n",
    "# Determine the number of samples and classes from the shape of the training data and labels.\n",
    "# This will help in setting up the network architecture later.\n",
    "Nsamps, Nclasses = X.shape[-1], labels.shape[-1]\n",
    "# Convert numpy arrays to PyTorch tensors. Tensors are a type of data structure used in PyTorch\n",
    "# that are similar to arrays.\n",
    "X_tensor, X_val_tensor = torch.FloatTensor(X), torch.FloatTensor(X_val)\n",
    "y_tensor, y_val_tensor = torch.FloatTensor(labels), torch.FloatTensor(labels_val)\n",
    "# Wrap tensors in a TensorDataset, which provides a way to access slices of tensors\n",
    "# using indexing that is useful during training because it abstracts away the data handling.\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataset_val = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "# DataLoader is used to efficiently load data in batches, which is necessary for training neural networks.\n",
    "# `shuffle=True` ensures that the data is shuffled at every epoch to prevent the model from learning\n",
    "# any order-based biases in the dataset.\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "194fb33e-af26-4d71-9f96-ac6351c93b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### Neural Network Architecture BluePrint ###########################################################\n",
    "\n",
    "# Define initial population (expanded blueprint with diverse hyperparameters)\n",
    "blueprint = {\n",
    "    'n_layers': 4,                                                 # specify a list of number of layers and neurons\n",
    "    'neurons': [512, 256, 128, 64, 32],                             # specify number of neurons per each layer\n",
    "    'activations': ['Leaky ReLU', 'ReLU', 'tanh', 'softmax'],       # sopecify of popular activation functions to choose from\n",
    "    'dropout': [None, 0.1, 0.2, 0.3, 0.4, 0.01]                     # specify different dropout options for regularization\n",
    "    \n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "145a957c-b23f-406e-9f83-f42666e1de01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Epoch 1/5 complete. Average Training Loss: 1.8063\n",
      "Epoch 2/5 complete. Average Training Loss: 1.7958\n",
      "Epoch 3/5 complete. Average Training Loss: 1.7934\n",
      "Epoch 4/5 complete. Average Training Loss: 1.7894\n",
      "Epoch 5/5 complete. Average Training Loss: 1.7851\n",
      "Training complete.\n",
      "Evaluation complete. Accuracy: 0.1922 (69/359 correct)\n",
      "\n",
      "    Genome 1/5 evaluated. \"Fitness\" (i.e. accuracy): 0.1922.\n",
      "\n",
      "Epoch 1/5 complete. Average Training Loss: 1.8181\n",
      "Epoch 2/5 complete. Average Training Loss: 1.8109\n",
      "Epoch 3/5 complete. Average Training Loss: 1.8065\n",
      "Epoch 4/5 complete. Average Training Loss: 1.7996\n",
      "Epoch 5/5 complete. Average Training Loss: 1.7946\n",
      "Training complete.\n",
      "Evaluation complete. Accuracy: 0.1727 (62/359 correct)\n",
      "\n",
      "    Genome 2/5 evaluated. \"Fitness\" (i.e. accuracy): 0.1727.\n",
      "\n",
      "Epoch 1/5 complete. Average Training Loss: 1.8020\n",
      "Epoch 2/5 complete. Average Training Loss: 1.7922\n",
      "Epoch 3/5 complete. Average Training Loss: 1.7751\n",
      "Epoch 4/5 complete. Average Training Loss: 1.7491\n",
      "Epoch 5/5 complete. Average Training Loss: 1.6405\n",
      "Training complete.\n",
      "Evaluation complete. Accuracy: 0.2284 (82/359 correct)\n",
      "\n",
      "    Genome 3/5 evaluated. \"Fitness\" (i.e. accuracy): 0.2284.\n",
      "\n",
      "Epoch 1/5 complete. Average Training Loss: 1.8007\n",
      "Epoch 2/5 complete. Average Training Loss: 1.7946\n",
      "Epoch 3/5 complete. Average Training Loss: 1.7926\n",
      "Epoch 4/5 complete. Average Training Loss: 1.7837\n",
      "Epoch 5/5 complete. Average Training Loss: 1.7682\n",
      "Training complete.\n",
      "Evaluation complete. Accuracy: 0.1950 (70/359 correct)\n",
      "\n",
      "    Genome 4/5 evaluated. \"Fitness\" (i.e. accuracy): 0.1950.\n",
      "\n",
      "Epoch 1/5 complete. Average Training Loss: 1.7987\n",
      "Epoch 2/5 complete. Average Training Loss: 1.7886\n",
      "Epoch 3/5 complete. Average Training Loss: 1.7723\n",
      "Epoch 4/5 complete. Average Training Loss: 1.6753\n",
      "Epoch 5/5 complete. Average Training Loss: 1.5323\n",
      "Training complete.\n",
      "Evaluation complete. Accuracy: 0.2897 (104/359 correct)\n",
      "\n",
      "    Genome 5/5 evaluated. \"Fitness\" (i.e. accuracy): 0.2897.\n",
      "\n",
      "All genomes in generation 0 have been evaluated.\n",
      "Generation 1, Best Fitness: 0.28969359331476324\n",
      "Best Architecture: [{'num_neurons': 32, 'activation': 'softmax', 'dropout_rate': 0.1}, {'num_neurons': 128, 'activation': 'tanh', 'dropout_rate': 0.01}, {'num_neurons': 256, 'activation': 'tanh', 'dropout_rate': 0.2}, {'num_neurons': 32, 'activation': 'Leaky ReLU', 'dropout_rate': 0.1}] \n",
      "\n",
      "Epoch 1/5 complete. Average Training Loss: 1.7990\n",
      "Epoch 2/5 complete. Average Training Loss: 1.7907\n",
      "Epoch 3/5 complete. Average Training Loss: 1.7820\n",
      "Epoch 4/5 complete. Average Training Loss: 1.7608\n",
      "Epoch 5/5 complete. Average Training Loss: 1.7233\n",
      "Training complete.\n",
      "Evaluation complete. Accuracy: 0.2145 (77/359 correct)\n",
      "\n",
      "    Genome 1/5 evaluated. \"Fitness\" (i.e. accuracy): 0.2145.\n",
      "\n",
      "Epoch 1/5 complete. Average Training Loss: 1.8013\n",
      "Epoch 2/5 complete. Average Training Loss: 1.7921\n",
      "Epoch 3/5 complete. Average Training Loss: 1.7737\n",
      "Epoch 4/5 complete. Average Training Loss: 1.7311\n",
      "Epoch 5/5 complete. Average Training Loss: 1.6604\n",
      "Training complete.\n",
      "Evaluation complete. Accuracy: 0.2702 (97/359 correct)\n",
      "\n",
      "    Genome 2/5 evaluated. \"Fitness\" (i.e. accuracy): 0.2702.\n",
      "\n",
      "Epoch 1/5 complete. Average Training Loss: 1.8003\n",
      "Epoch 2/5 complete. Average Training Loss: 1.7928\n",
      "Epoch 3/5 complete. Average Training Loss: 1.7737\n",
      "Epoch 4/5 complete. Average Training Loss: 1.7083\n",
      "Epoch 5/5 complete. Average Training Loss: 1.6245\n",
      "Training complete.\n",
      "Evaluation complete. Accuracy: 0.2646 (95/359 correct)\n",
      "\n",
      "    Genome 3/5 evaluated. \"Fitness\" (i.e. accuracy): 0.2646.\n",
      "\n",
      "Epoch 1/5 complete. Average Training Loss: 1.7970\n",
      "Epoch 2/5 complete. Average Training Loss: 1.7873\n",
      "Epoch 3/5 complete. Average Training Loss: 1.7692\n",
      "Epoch 4/5 complete. Average Training Loss: 1.7143\n",
      "Epoch 5/5 complete. Average Training Loss: 1.6339\n",
      "Training complete.\n",
      "Evaluation complete. Accuracy: 0.2813 (101/359 correct)\n",
      "\n",
      "    Genome 4/5 evaluated. \"Fitness\" (i.e. accuracy): 0.2813.\n",
      "\n",
      "Epoch 1/5 complete. Average Training Loss: 1.7986\n",
      "Epoch 2/5 complete. Average Training Loss: 1.7931\n",
      "Epoch 3/5 complete. Average Training Loss: 1.7843\n",
      "Epoch 4/5 complete. Average Training Loss: 1.7760\n",
      "Epoch 5/5 complete. Average Training Loss: 1.7661\n",
      "Training complete.\n",
      "Evaluation complete. Accuracy: 0.2256 (81/359 correct)\n",
      "\n",
      "    Genome 5/5 evaluated. \"Fitness\" (i.e. accuracy): 0.2256.\n",
      "\n",
      "All genomes in generation 1 have been evaluated.\n",
      "Generation 2, Best Fitness: 0.28133704735376047\n",
      "Best Architecture: [{'num_neurons': 32, 'activation': 'ReLU', 'dropout_rate': 0.3}, {'num_neurons': 32, 'activation': 'tanh', 'dropout_rate': 0.01}, {'num_neurons': 256, 'activation': 'tanh', 'dropout_rate': 0.2}, {'num_neurons': 32, 'activation': 'Leaky ReLU', 'dropout_rate': 0.1}] \n",
      "\n",
      "\n",
      "Final Summary\n",
      "Best Overall Fitness: 0.28969359331476324\n",
      "Best Overall Architecture: [{'num_neurons': 32, 'activation': 'softmax', 'dropout_rate': 0.1}, {'num_neurons': 32, 'activation': 'Leaky ReLU', 'dropout_rate': 0.4}, {'num_neurons': 256, 'activation': 'tanh', 'dropout_rate': 0.2}, {'num_neurons': 32, 'activation': 'Leaky ReLU', 'dropout_rate': 0.1}]\n",
      "\n",
      "Starting the re-training of the best model found by the genetic algorithm (corroborate reproducibility)\n",
      "Epoch 1/5 complete. Average Training Loss: 1.7990\n",
      "Epoch 2/5 complete. Average Training Loss: 1.7919\n",
      "Epoch 3/5 complete. Average Training Loss: 1.7798\n",
      "Epoch 4/5 complete. Average Training Loss: 1.7700\n",
      "Epoch 5/5 complete. Average Training Loss: 1.7470\n",
      "Evaluation on validation set complete. Accuracy: 0.2228 (80/359 correct)\n",
      "Saved the best model's weights to 'best_net1.pth'\n"
     ]
    }
   ],
   "source": [
    "######################################### Optimization, loss, model saving ###########################################################\n",
    "\n",
    "\n",
    "population = generate_initial_population(population_size, blueprint)\n",
    "\n",
    "# Initialize best performance tracking\n",
    "best_overall_fitness = float('-inf')\n",
    "best_overall_architecture = None\n",
    "\n",
    "for generation in range(num_generations):\n",
    "\n",
    "    # Evaluate fitnesses\n",
    "    fitnesses = []\n",
    "    total_genomes = len(population)\n",
    "    for idx, genome in enumerate(population):\n",
    "        # Compute the fitness for each genome\n",
    "        fitness = compute_fitness(genome, train_loader, val_loader, nn.CrossEntropyLoss(), lr=LR, epochs=EPOCHS, D=Nsamps, K=Nclasses)\n",
    "        fitnesses.append(fitness)\n",
    "        print(f'    Genome {idx + 1}/{total_genomes} evaluated. \"Fitness\" (i.e. accuracy): {fitness:.4f}.\\n')\n",
    "    print(f\"All genomes in generation {generation} have been evaluated.\")\n",
    "\n",
    "    parents = selection(population, fitnesses, num_parents)\n",
    "\n",
    "    # Track the best architecture in this generation\n",
    "    max_fitness_idx = fitnesses.index(max(fitnesses))\n",
    "    best_fitness_this_gen = fitnesses[max_fitness_idx]\n",
    "    best_architecture_this_gen = population[max_fitness_idx]\n",
    "\n",
    "    # Update overall best if the current gen has a new best\n",
    "    if best_fitness_this_gen > best_overall_fitness:\n",
    "        best_overall_fitness = best_fitness_this_gen\n",
    "        best_overall_architecture = best_architecture_this_gen\n",
    "\n",
    "    print(f\"Generation {generation + 1}, Best Fitness: {best_fitness_this_gen}\")\n",
    "    print(\"Best Architecture:\", best_architecture_this_gen, '\\n')\n",
    "\n",
    "    # Generate next generation\n",
    "    next_generation = parents[:]\n",
    "    while len(next_generation) < population_size:\n",
    "        parent1, parent2 = random.sample(parents, 2)\n",
    "        child = crossover(parent1, parent2)\n",
    "        #child = mutate(child)\n",
    "        child = mutate(child, mutation_rate=0.1, blueprint=blueprint)\n",
    "        next_generation.append(child)\n",
    "    population = next_generation\n",
    "\n",
    "# You do not need to change anything here\n",
    "\n",
    "# Final summary at the end of all generations\n",
    "print(\"\\nFinal Summary\")\n",
    "print(\"Best Overall Fitness:\", best_overall_fitness)\n",
    "print(\"Best Overall Architecture:\", best_overall_architecture)\n",
    "\n",
    "# Inform about the beginning of the re-training process\n",
    "print(\"\\nStarting the re-training of the best model found by the genetic algorithm (corroborate reproducibility)\")\n",
    "\n",
    "# Re-build the best model based on the architecture determined to be most effective during the genetic algorithm.\n",
    "# This model is built from scratch using the best configuration parameters (genome) found.\n",
    "best_model = Net(best_overall_architecture, D=Nsamps, K=Nclasses)\n",
    "\n",
    "# Set up the loss function and the optimizer. The optimizer is configured to optimize the weights of our neural network,\n",
    "# and the learning rate is set as per earlier specification.\n",
    "best_model_criterion = nn.CrossEntropyLoss()\n",
    "best_model_optimizer = optim.Adam(best_model.parameters(), lr=LR)\n",
    "\n",
    "# Training loop: This process involves multiple epochs where each epoch goes through the entire training dataset.\n",
    "for epoch in range(EPOCHS):\n",
    "    best_model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_batches = len(train_loader)\n",
    "\n",
    "    # Process each batch of data\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        best_model_optimizer.zero_grad()  # Clear previous gradients\n",
    "        output = best_model(data)  # Compute the model's output\n",
    "        loss = best_model_criterion(output, target)  # Calculate loss\n",
    "        loss.backward()  # Compute gradients\n",
    "        best_model_optimizer.step()  # Update weights\n",
    "        total_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "    average_epoch_loss = total_loss / total_batches\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS} complete. Average Training Loss: {average_epoch_loss:.4f}')\n",
    "\n",
    "# After training, switch to evaluation mode for testing.\n",
    "best_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Disable gradient computation for validation, as it isn't needed and saves memory and computation.\n",
    "with torch.no_grad():\n",
    "    # Process each batch from the validation set\n",
    "    for data, target in val_loader:\n",
    "        output = best_model(data)  # Compute the model's output\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # Find the predicted class\n",
    "        target = target.argmax(dim=1, keepdim=True)  # Actual class\n",
    "        correct += pred.eq(target).sum().item()  # Count correct predictions\n",
    "        total += target.size(0)  # Total number of items\n",
    "\n",
    "validation_accuracy = correct / total  # Calculate accuracy\n",
    "print(f'Evaluation on validation set complete. Accuracy: {validation_accuracy:.4f} ({correct}/{total} correct)')\n",
    "\n",
    "# Save the trained model's weights for future use.\n",
    "torch.save(best_model.state_dict(), 'best_net1.pth')\n",
    "print(\"Saved the best model's weights to 'best_net1.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be306e7-d432-4901-a576-2289713e462f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
